{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ad1396c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic OpenAI-compatible endpoint using ChatOpenAI\n",
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "import os\n",
    "default_model_name = os.environ[\"OPENAI_MODEL\"]\n",
    "pg_connection = os.environ[\"PGVECTOR_CONNECTION_STRING\"]\n",
    "llm = ChatOpenAI(model=default_model_name,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82cee803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berhasil konek dengan collection 'langchain'\n"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings \n",
    "from langchain_postgres.vectorstores import PGVector\n",
    " \n",
    " \n",
    "# embed each chunk and insert it into the vector store\n",
    "embeddings_model = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "# Coba dengan nama default\n",
    "try:\n",
    "    db = PGVector.from_existing_index(\n",
    "        embedding=embeddings_model,\n",
    "        collection_name=\"langchain\",  # Coba default\n",
    "        connection=pg_connection,\n",
    "    )\n",
    "    print(\"Berhasil konek dengan collection 'langchain'\")\n",
    "except Exception as e1:\n",
    "    print(f\"Collection 'langchain' tidak ditemukan: {e1}\")\n",
    "    \n",
    "    # Coba dengan nama lain yang mungkin\n",
    "    try:\n",
    "        db = PGVector.from_existing_index(\n",
    "            embedding=embeddings_model,\n",
    "            collection_name=\"nomic-embed-text\",  # Nama model\n",
    "            connection=pg_connection,\n",
    "        )\n",
    "        print(\"Berhasil konek dengan collection 'nomic-embed-text'\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Collection 'nomic-embed-text' tidak ditemukan: {e2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a40c715e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1677ab38-7604-4515-869a-ddf06cf5bae8', metadata={'source': '../../tesla.md'}, page_content='---\\n\\n PART III\\n\\nITEM 10\\\\. DIRECTORS, EXECUTIVE OFFICERS AND CORPORATE GOVERNANCE\\n\\nThe information required by this Item 10 of Form 10-K will be included in our 2023 Proxy Statement to be filed with the Securities and Exchange Commission in connection with the solicitation of proxies for our 2023 Annual Meeting of Stockholders and is incorporated herein by reference. The 2023 Proxy Statement will be filed with the Securities and Exchange Commission within 120 days after the end of the fiscal year to which this report relates. \\n\\nITEM 11\\\\. EXECUTIVE COMPENSATION\\n\\nThe information required by this Item 11 of Form 10-K will be included in our 2023 Proxy Statement and is incorporated herein by reference. \\n\\nITEM 12\\\\. SECURITY OWNERSHIP OF CERTAIN BENEFICIAL OWNERS AND MANAGEMENT AND RELATED STOCKHOLDER MATTERS\\n\\nThe information required by this Item 12 of Form 10-K will be included in our 2023 Proxy Statement and is incorporated herein by reference.'),\n",
       " Document(id='980bb6a3-b27a-4c24-b797-1edeaca1cd99', metadata={'source': '../../tesla.md'}, page_content='---\\n\\n Note 3 â€“ Digital Assets, Net'),\n",
       " Document(id='1d371111-68cd-4c63-a54b-e9c0600e3516', metadata={'source': '../../tesla.md'}, page_content='and varying levels of inflation, we currently expect our capital expenditures to be between $6.00 to $8.00 billion in 2023 and between $7.00 to $9.00 billion in each of the following two fiscal years.'),\n",
       " Document(id='e97c37f8-9ca4-4ea9-859f-1946862005f9', metadata={'source': '../../tesla.md'}, page_content='| 10.48   |  | [Third Amendment to Amended and Restated Agreement For Research & Development Alliance on Triex Module Technology, effective as of February 12, 2015, by and between The Research Foundation For The State University of New York, on behalf of the College of Nanoscale Science and Engineering of the State University of New York, and Silevo, Inc.](https://www.sec.gov/Archives/edgar/data/1408356/000156459015003352/scty-ex1016c%5F20150331365.htm) |  | 10-Q(1)                   |  | 001-35758 |  | 10.16c  |  | May 6, 2015      |  |          |')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.similarity_search(\"Q3\", k=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "195c0001",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q:Daftar Produk Tesla lengkap\n",
      "Berdasarkan dokumen yang tersedia, produk Tesla meliputi:\n",
      "\n",
      "1. **Kendaraan listrik (mobil)** - Termasuk kendaraan baru dan bekas yang dijual oleh Tesla, dilengkapi dengan jaminan terbatas pabrikan.\n",
      "2. **Sistem penyimpanan energi** - Produk yang berkaitan dengan pembangkitan dan penyimpanan energi, seperti baterai rumah atau sistem penyimpanan energi lainnya.\n",
      "3. **Layanan perluasan jaminan dan rencana layanan tambahan** - Untuk kendaraan dan produk energi tertentu, Tesla menawarkan paket layanan tambahan yang dapat dibeli untuk memperpanjang cakupan garansi.\n",
      "\n",
      "Selain itu, Tesla juga menyediakan layanan perbaikan dan perawatan untuk produk-produk tersebut, termasuk layanan instalasi untuk sistem energi, lengkap dengan jaminan terhadap pekerjaan instalasi tersebut.\n"
     ]
    }
   ],
   "source": [
    "# create retriever\n",
    "# retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Always answer in indonesian language.Answer the question based only on\n",
    "    the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n",
    "chain = prompt | llm\n",
    "rewrite_prompt = ChatPromptTemplate.from_template(\"\"\"Provide a better search\n",
    "    query for web search engine to answer the given question, end the queries\n",
    "    with '**'.\n",
    "    examples:\n",
    "    Question: Minyak\n",
    "    Answer: Jenis Minyak**\n",
    "    Question: {x} Answer:\"\"\")\n",
    "\n",
    "def parse_rewriter_output(message):\n",
    "    return message.content.strip('\"').strip(\"**\")\n",
    "\n",
    "rewriter = rewrite_prompt | llm | parse_rewriter_output\n",
    "\n",
    " \n",
    "def qa_rrr(input):\n",
    "    # rewrite the query\n",
    "    new_query = rewriter.invoke(input)\n",
    "    print(f\"q:{new_query}\")\n",
    "    # fetch relevant documents\n",
    "    docs = retriever.get_relevant_documents(new_query)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    " \n",
    "result=qa_rrr(\"Apa saja produk TESLA?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d6913a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cabai hijau adalah jenis cabai yang dipanen pada tahap kematangan awal, sebelum terjadi perubahan warna. Berdasarkan informasi dari dokumen, cabai hijau biasanya dipanen sekitar 70-80 hari setelah tanam. Pemanenan dilakukan dengan cara memotong tangkai buah menggunakan gunting atau pisau tajam, dan sebaiknya dilakukan di pagi hari setelah embun kering atau di sore hari untuk mengurangi stres pada tanaman.\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "perspectives_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are an AI language model assistant. Your task is to generate five different versions \n",
    "    of the given user question to retrieve relevant documents from a vector database.\n",
    "    By generating multiple perspectives on the user question, your goal is to help the user \n",
    "    overcome some of the limitations of the distance-based similarity search. Provide these \n",
    "    alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    ")\n",
    "def parse_queries_output(message):\n",
    "    return message.content.split('\\n')\n",
    "\n",
    "query_gen = perspectives_prompt | llm | parse_queries_output\n",
    "\n",
    "def get_unique_union(document_lists):\n",
    "    # Flatten list of lists, and dedupe them\n",
    "    deduped_docs = {\n",
    "        doc.page_content: doc\n",
    "        for sublist in document_lists for doc in sublist\n",
    "    }\n",
    "    # return a flat list of unique docs\n",
    "    return list(deduped_docs.values())\n",
    "\n",
    "retrieval_chain = query_gen | retriever.batch | get_unique_union\n",
    "\n",
    "def multi_query_qa(input):\n",
    "    # fetch relevant documents\n",
    "    docs = retrieval_chain.invoke(input)\n",
    "    # format prompt\n",
    "    formatted = prompt.invoke({\"context\": docs, \"question\": input})\n",
    "    # generate answer\n",
    "    answer = llm.invoke(formatted)\n",
    "    return answer\n",
    "\n",
    "result=multi_query_qa(\"cabe hujau?\")\n",
    "print(result.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning-langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
