{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "generic-configuration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Promise { <pending> }"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TypeError [ERR_VM_DYNAMIC_IMPORT_CALLBACK_MISSING]: A dynamic import callback was not specified.\n",
      "    at importModuleDynamicallyCallback (node:internal/modules/esm/utils:272:9)\n",
      "    at evalmachine.<anonymous>:2:46\n",
      "    at evalmachine.<anonymous>:23:3\n",
      "    at Script.runInThisContext (node:vm:137:12)\n",
      "    at Object.runInThisContext (node:vm:317:38)\n",
      "    at run ([eval]:1020:15)\n",
      "    at onRunRequest ([eval]:864:18)\n",
      "    at onMessage ([eval]:828:13)\n",
      "    at process.emit (node:events:524:28)\n",
      "    at process.emit (node:domain:489:12)"
     ]
    }
   ],
   "source": [
    "( async()=>{\n",
    "    const  { HumanMessage, SystemMessage } = await import( \"@langchain/core/messages\")\n",
    "const { ChatOpenAI } = await import( \"@langchain/openai\")\n",
    "const { config } = await import(\"dotenv\")\n",
    "config()\n",
    "const model = new ChatOpenAI({\n",
    "    model: process.env.OPENAI_MODEL,\n",
    "    temperature: 0.5,\n",
    "    // maxTokens: 100,\n",
    "  })\n",
    "\n",
    "  //   const result = await model.invoke(\"The sky is\")\n",
    "  const prompt = [\n",
    "    new SystemMessage(\n",
    "      `You are a helpful assistant that responds to questions with three \n",
    "      exclamation marks.`\n",
    "    ),\n",
    "    new HumanMessage(\"What is the capital of France?\"),\n",
    "  ]\n",
    "\n",
    "  const result = await model.invoke(prompt)\n",
    "  console.log(result.content)\n",
    "})()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d58b398",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is **Paris**.', response_metadata={'token_usage': {'completion_tokens': 13, 'prompt_tokens': 13, 'total_tokens': 27, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 7}, 'words': 52}, 'model_name': 'zai', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-f47a941d-fd4f-4932-83c5-4de600cc733c-0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [HumanMessage(\"What is the capital of France?\")]\n",
    "llm.invoke(messages) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7c598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='The capital of France is Paris!!!', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 32, 'total_tokens': 44, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 4}, 'words': 50}, 'model_name': 'zai', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-16b4fee3-44e8-4b66-ae02-9a95642c0913-0')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a0200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae304344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berdasarkan konteks yang diberikan, penyedia model yang menawarkan LLM antara lain:\n",
      "\n",
      "1. **Hugging Face** (melalui library `transformers`)\n",
      "2. **OpenAI** (melalui library `openai`)\n",
      "3. **Cohere** (melalui library `cohere`)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ac20e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Berdasarkan konteks yang diberikan, penyedia model yang menawarkan LLM antara lain:\n",
      "\n",
      "1. **Hugging Face** - melalui library `transformers`.\n",
      "2. **OpenAI** - melalui layanan dan library `openai`.\n",
      "3. **Cohere** - melalui layanan dan library `cohere`.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Jawab pertanyaan berdasarkan konteks\n",
    "    di bawah ini. Jika pertanyaan tidak dapat dijawab menggunakan informasi\n",
    "    yang diberikan, jawab dengan \"Saya tidak tahu\".\n",
    "\n",
    "Konteks: {context}\n",
    "\n",
    "Pertanyaan: {question}\n",
    "\n",
    "Jawaban: \"\"\")\n",
    "\n",
    "result=chain.invoke({\n",
    "    \"context\":context,\n",
    "    \"question\":question\n",
    "})\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eae870b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "JawabanDenganJustifikasi(jawaban='Keduanya sama berat.', justifikasi='Satu pon batu bata dan satu pon bulu memiliki berat yang sama karena satuan \"pon\" menyatakan berat yang identik, terlepas dari jenis materialnya. Meskipun volume bulu jauh lebih besar daripada batu bata, berat keduanya tetap sama yaitu satu pon.')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "\n",
    "class JawabanDenganJustifikasi(BaseModel):\n",
    "    '''Jawaban atas pertanyaan pengguna beserta justifikasi untuk jawaban tersebut.'''\n",
    "    jawaban: str\n",
    "    '''Jawaban atas pertanyaan pengguna'''\n",
    "    justifikasi: str\n",
    "    '''Justifikasi untuk jawaban'''\n",
    "\n",
    "structured_llm = llm.with_structured_output(JawabanDenganJustifikasi)\n",
    "\n",
    "structured_llm.invoke(\"\"\"Mana yang lebih berat, satu pon batu bata atau satu pon bulu?\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JavaScript (Node.js)",
   "language": "javascript",
   "name": "javascript"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "application/javascript",
   "name": "javascript",
   "version": "20.19.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
